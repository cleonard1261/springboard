{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import os \n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26 17:15:10.046771\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chadleonard/Springboard/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Predicting Star Ratings\n",
    "\n",
    "Our objective is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a dataset of venue popularities provided by Yelp.  The dataset contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. Note that the venues are not limited to restaurants. This tutorial will walk you through one way to build a machine-learning algorithm.\n",
    "\n",
    "## Metric\n",
    "\n",
    "Your model will be assessed based on the root mean squared error of the number of stars you predict.  There is a reference solution (which should not be too hard to beat).  The reference solution has a score of 1. Keeping this in mind...\n",
    "\n",
    "## A note on scoring\n",
    "It **is** possible to score >1 on these questions. This indicates that you've beaten our reference model - we compare our model's score on a test set to your score on a test set. See how high you can go!\n",
    "\n",
    "\n",
    "## Download and parse the incoming data\n",
    "\n",
    "We start by downloading the dataset from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: aws: command not found\r\n"
     ]
    }
   ],
   "source": [
    "#!/Users/chadleonard/.local/bin/aws s3 sync s3://dataincubator-course/mldata/ . \\\n",
    "#      --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'\n",
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chadleonard/DataIncubatorFall2017/notebooks/machine_learning'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a gzipped file. Python supports gzipped files natively: [gzip.open](https://docs.python.org/2/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in json package has a `loads()` function that converts a JSON string into a Python dictionary.  We could call that once for each row of the file. [ujson](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` library, but is *substantially* faster (at the cost of non-robust handling of malformed json).  We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26 18:24:45.768581\n",
      "(188593, 15)\n",
      "2018-10-26 18:25:16.707357\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(dt.datetime.now())\n",
    "f = '/Users/chadleonard/Springboard/work/springboard/data/yelp_academic_dataset_business.json'\n",
    "\n",
    "json_list = []\n",
    "df = pd.DataFrame([json.loads(line) for line in open(f, 'r')])\n",
    "print(df.shape)\n",
    "\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1314 44 Avenue NE</td>\n",
       "      <td>{'BikeParking': 'False', 'BusinessAcceptsCredi...</td>\n",
       "      <td>Apn5Q_b6Nz61Tq4XzPdf9A</td>\n",
       "      <td>Tours, Breweries, Pizza, Restaurants, Food, Ho...</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>{'Monday': '8:30-17:0', 'Tuesday': '11:0-21:0'...</td>\n",
       "      <td>1</td>\n",
       "      <td>51.091813</td>\n",
       "      <td>-114.031675</td>\n",
       "      <td>Minhas Micro Brewery</td>\n",
       "      <td></td>\n",
       "      <td>T2E 6L6</td>\n",
       "      <td>24</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             address                                         attributes  \\\n",
       "0  1314 44 Avenue NE  {'BikeParking': 'False', 'BusinessAcceptsCredi...   \n",
       "\n",
       "              business_id                                         categories  \\\n",
       "0  Apn5Q_b6Nz61Tq4XzPdf9A  Tours, Breweries, Pizza, Restaurants, Food, Ho...   \n",
       "\n",
       "      city                                              hours  is_open  \\\n",
       "0  Calgary  {'Monday': '8:30-17:0', 'Tuesday': '11:0-21:0'...        1   \n",
       "\n",
       "    latitude   longitude                  name neighborhood postal_code  \\\n",
       "0  51.091813 -114.031675  Minhas Micro Brewery                  T2E 6L6   \n",
       "\n",
       "   review_count  stars state  \n",
       "0            24    4.0    AB  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6315504817251965"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_ratings = [row for row in df['stars']]\n",
    "\n",
    "sum(star_ratings) / len(star_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit Learn, the labels to be predicted, in this case, the stars, are always kept in a separate data structure than the features.  Let's get in this habit now, by creating a separate list of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6729137013021247"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_ratings = [row['stars'] for row in data]\n",
    "\n",
    "sum(star_ratings) / len(star_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "1. [Pandas](http://pandas.pydata.org/) is able to read JSON text directly.  Use the `read_json()` function with the `lines=True` keyword argument.  While the rest of this notebook will assume you are using a list of dictionaries, you can complete it with dataframes, if you so desire.  Some of the example code will need to be modified in this case.\n",
    "\n",
    "2. There are obvious miscodings in the data.  There is no need to try to correct them.\n",
    "\n",
    "## Building models\n",
    "\n",
    "For many of the questions below, you will need to build and train an estimator that predicts the star rating given certain features.  This could be a custom estimator that you built from scratch, but in most cases will be a pipeline containing custom or prebuilt transformers and an existing estimator.  We will give you hints of how to proceed, but the only requirement for you is to produce a model that does as well, or better, than the reference models we created.  You are welcome to do this however you like. The details are up to you.\n",
    "\n",
    "The formats of the input and output to the `fit()` and `predict()` methods are ultimately up to you as well, but we recommend that you deal with lists or arrays, for consistency with the rest of Scikit Learn.  It is also a good idea to take the same type of data for the feature matrix in both `fit()` and `predict()`.  While it is tempting to read the stars from the feature matrix X, you should get in the habit of passing the labels as a separate argument to the `fit()` method.\n",
    "\n",
    "You may find it useful to serialize the trained models to disk.  This will allow to reload it after restarting the Jupyter notebook, without needing to retrain it.  We recommend using the [`dill` library](https://pypi.python.org/pypi/dill) for this (although the [`joblib` library](http://scikit-learn.org/stable/modules/model_persistence.html) also works).  Use\n",
    "```python\n",
    "dill.dump(estimator, open('estimator.dill', 'w'))\n",
    "```\n",
    "to serialize the object `estimator` to the file `estimator.dill`.  If you have trouble with this, try setting the `recurse=True` keyword args in the call of `dill.dump()`.  The estimator can be deserialized by calling\n",
    "```python\n",
    "estimator = dill.load(open('estimator.dill', 'r'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Each of the \"model\" questions asks you to create a function that models the number of stars venues will receive.  It will be passed a list of dictionaries.  Each of these will have the same format as the JSON objects you've just read in.  Some of the keys (like the stars!) will have been removed.  This function should return a list of numbers of the same length, indicating the predicted star ratings.\n",
    "\n",
    "This function is passed to the `score()` function, which will receive input from the grader, run your function with that input, report the results back to the grader, and print out the score the grader returned.  Depending on how you constructed your estimator, you may be able to pass the predict method directly to the `score()` function.  If not, you will need to write a small wrapper function to mediate the data types.\n",
    "\n",
    "## city_avg\n",
    "The venues belong to different cities.  You can image that the ratings in some cities are probably higher than others.  We wish to build an estimator to make a prediction based on this, but first we need to work out the average rating for each city.  For this problem, create a list of tuples (city name, star rating), one for each city in the dataset.\n",
    "\n",
    "There are many ways to do this; please feel free to experiment on your own.  If you get stuck, the steps below attempt to guide you through the process.\n",
    "\n",
    "A simple approach is to go through all of the dictionaries in our array, calculating the sum of the star ratings and the number of venues for each city.  At the end, we can just divide the stars by the count to get the average.\n",
    "\n",
    "We could create a separate sum and count variable for each city, but that will get tedious quickly.  A better approach to to create a dictionary for each.  The key will be the city name, and the value the running sum or running count.\n",
    "\n",
    "One slight annoyance of this approach is that we will have to test whether a key exists in the dictionary before adding to the running tally.  The collections module's `defaultdict` class works around this by providing default values for keys that haven't been used.  Thus, if we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "star_sum = defaultdict(int)\n",
    "count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can increment any key of `stars` or `count` without first worrying whether the key exists.  We need to go through the `data` and `star_ratings` list together, which we can do with the `zip()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row, stars in zip(data, star_ratings):\n",
    "    for a in [row.get('city')]:\n",
    "        a = '+'+a+'+'\n",
    "        count[a] += 1\n",
    "        star_sum[a] += stars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the average ratings.  Again, a dictionary makes a good container.  (*N.B.* Watch out for Python2's integer division.  Make sure at least one of your values is a float to get floating point division.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_stars = dict()\n",
    "for city in star_sum:\n",
    "    # calculate average star rating and store in avg_stars\n",
    "    avg_stars[city.replace('+','')] = (star_sum[city] / count[city])\n",
    "\n",
    "avg_stars\n",
    "len(avg_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 167 different cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(avg_stars) == 167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get that list of tuples from the `.items()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__city_avg', lambda: avg_stars.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city_model\n",
    "Now, let's build a custom estimator that will make a prediction based solely on the city of a venue.  It is tempting to hard-code the answers from the previous section into this model, but we're going to resist and do things properly.\n",
    "\n",
    "This custom estimator will have a `.fit()` method.  It will receive `data` as its argument `X` and `star_ratings` as `y`, and should repeat the calculation of the previous problem there.  Then the `.predict()` method can look up the average rating for the city of each record it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class CityEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = dict()\n",
    "        self.star_sum = defaultdict(int)\n",
    "        self.count = defaultdict(int)\n",
    "        self.med = 0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store the average rating per city in self.avg_stars\n",
    "        for row, stars in zip(X, y):\n",
    "            self.count[row['city']] += 1\n",
    "            self.star_sum[row['city']] += stars\n",
    "        for city in self.count:\n",
    "            self.avg_stars[city] = (self.star_sum[city] / self.count[city])\n",
    "        self.med = np.median(self.avg_stars.values())\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.avg_stars[row['city']] if row['city'] in self.avg_stars else self.med for row in X ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an instance of our estimator and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityEstimator()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_est = CityEstimator()\n",
    "city_est.fit(data, star_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem, however.  What happens if we're asked to estimate the rating of a venue in a city that's not in our training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6456043956043955]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_est.predict([{'city': 'Timbuktu'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve this problem before submitting to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__city_model', city_est.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lat_long_model\n",
    "You can imagine that a city-based model might not be sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  Use the latitude and longitude of a venue as features that help you understand neighborhood dynamics.\n",
    "\n",
    "Instead of writing a custom estimator, we'll use one of the built-in estimators in Scikit Learn.  Since these estimators won't know what to do with a list of dictionaries, we'll build a `ColumnSelectTransformer` that will return an array containing selected keys of our feature matrix.  While it is tempting to hard-code the latitude and longitude in here, this transformer will be more useful in the future if we write it to work on an arbitrary list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and one\n",
    "        # column for each in self.col_names\n",
    "        result = []\n",
    "        #print X[:4]\n",
    "        for l in X:\n",
    "            result.append([l.get(key) for key in self.col_names])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a single row, just as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cst = ColumnSelectTransformer(['latitude', 'longitude'])\n",
    "assert (cst.fit_transform(data[:1])\n",
    "        == [[data[0]['latitude'], data[0]['longitude']]])\n",
    "\n",
    "type(cst.fit_transform(data[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed the output of the transformer in to a `sklearn.neighbors.KNeighborsRegressor`.  As a sanity check, we'll test it with the first 5 rows.  To truly judge the performance, we'd need to make a test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "n=50\n",
    "cols = ['stars']\n",
    "truescore = [l.get(key) for key in cols for l in data[:n]]\n",
    "data_transform = cst.fit_transform(data)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights='distance', algorithm='auto', \\\n",
    "                    leaf_size=30, p=2,  metric='minkowski', metric_params=None, n_jobs=1)\n",
    "knn.fit(data_transform, star_ratings)\n",
    "test_data = data[:n]\n",
    "test_data_transform = cst.transform(test_data)\n",
    "\n",
    "print knn.score(test_data_transform,truescore)\n",
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance', metric='hamming')\n",
    "knn.fit(data_transform, star_ratings)\n",
    "test_data = data[:n]\n",
    "test_data_transform = cst.transform(test_data)\n",
    "print knn.score(test_data_transform,truescore)\n",
    "#knn.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this by hand, let's make a pipeline.  Remember that a pipeline is made with a list of (name, transformer-or-estimator) tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cols = ['latitude', 'longitude']\n",
    "\n",
    "pipe = Pipeline([\n",
    "   ('cst', ColumnSelectTransformer(cols)) , # ColumnSelectTransformer\n",
    "   ('knn', KNeighborsRegressor(n_neighbors=5,weights='distance',p=1, metric='minkowski')) # KNeighborsRegressor\n",
    "    ])\n",
    "\n",
    "n = len(data)\n",
    "print n\n",
    "pipe.fit(data[:20000], star_ratings[:20000])\n",
    "final = pipe.predict(data[:n])\n",
    "#print final[:15]\n",
    "print pipe.score(data[:n], star_ratings[:n])\n",
    "kdict = pipe.get_params()\n",
    "\n",
    "#print [(k,kdict[k]) for k in kdict ][0]# if isinstance(kdict[k], str)]\n",
    "print kdict['knn']\n",
    "print kdict['cst']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(data, star_ratings)\n",
    "pipe.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(data[:10], star_ratings[:10])\n",
    "print pipe.predict(data[:10])\n",
    "print star_ratings[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KNeighborsRegressor` takes the `n_neighbors` hyperparameter, which tells it how many nearest neighbors to average together when making a prediction.  There is no reason to believe that 5 is the optimum value.  Determine a better value of this hyperparameter.   There are several ways to do this:\n",
    "\n",
    "1. Use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) to split your data in to a training set and a test set.  Score the performance on the test set.  After finding the best hyperparameter, retrain the model on the full data at that hyperparameter value.\n",
    "\n",
    "2. Use [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to return cross-validation scores on your data for various values of the hyperparameter.  Choose the best one, and retrain the model on the full data.\n",
    "\n",
    "3. Use [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to do the splitting, training, and grading automatically.  `GridSearchCV` takes an estimator and acts as an estimator.  You can either give it the `KNeighborsRegressor` directly and put it in a pipeline, or you can pass the whole pipeline into the `GridSearchCV`.  In the latter case, remember that the hyperparameter `param` of an estimator named `est` in a pipeline becomes a hyperparameter of the pipeline with name `est__param`.\n",
    "\n",
    "No matter which you choose, you should consider whether the data need to be shuffled.  The default k-folds split doesn't shuffle.  This is fine, if the data are already random.  The code below will plot a rolling mean of the star ratings.  Do you need to shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Series.rolling(Series(star_ratings), window=1000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've found a good value of `n_neighbors`, submit the model to the grader.  (*N.B.* \"Good\" is a relative measure here.  The reference solution has a r-squared value of only 0.02.  There is just rather little signal available for modeling.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = dt.datetime.now()\n",
    "print st\n",
    "cols = ['longitude','latitude']\n",
    "\n",
    "# We are going to first scale and then apply KNeighbors.\n",
    "# Pipeline allows us to chain together multiple transformers and then a regressor or classifier\n",
    "pipe = Pipeline([\n",
    "   ('cst', ColumnSelectTransformer(cols)) , # ColumnSelectTransformer\n",
    "   ('scaling', preprocessing.StandardScaler(copy=True)),\n",
    "   ('knn', KNeighborsRegressor(weights='uniform')) # KNeighborsRegressor\n",
    "    ])\n",
    "\n",
    "cv = model_selection.ShuffleSplit(n_splits=20, test_size=0.2, random_state=42)\n",
    "print dt.datetime.now()\n",
    "# parameters to Pipeline take the form [label]__[estimator_param]\n",
    "param_grid = { \"knn__n_neighbors\": range(64, 100, 4) }\n",
    "lat_long_est = model_selection.GridSearchCV(pipe, \n",
    "                                                       param_grid=param_grid, cv=cv, \n",
    "                                                       scoring='r2')\n",
    "print dt.datetime.now()\n",
    "lat_long_est.fit(data, star_ratings)\n",
    "print dt.datetime.now()\n",
    "lat_long_est_results = lat_long_est.cv_results_\n",
    "lat_long_est_score = pd.DataFrame.from_items(\n",
    "    [('n_neighbors',[rec['knn__n_neighbors'] for rec in lat_long_est_results['params']]),\n",
    "     ('score',lat_long_est_results['mean_test_score'])])\n",
    "\n",
    "plt.plot(lat_long_est_score.n_neighbors, lat_long_est_score.score, label=\"lat_long_cv_score\")\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('score')\n",
    "plt.legend(loc='lower center')\n",
    "plt.show()\n",
    "\n",
    "st = dt.datetime.now()\n",
    "print st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat_long_est_results['mean_test_score']\n",
    "lat_long_est_results['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat_long_est_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__lat_long_model', lat_long_est.predict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Item for thought:* Why do we choose a non-linear model for this estimator?\n",
    "\n",
    "*Extension:* Use a `sklearn.ensemble.RandomForestRegressor`, which is a more powerful non-linear model.  Can you get better performance with this than with the `KNeighborsRegressor`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category_model\n",
    "While location is important, we could also try seeing how predictive the\n",
    "venue's category is.  Build an estimator that considers only the categories.\n",
    "\n",
    "The categories come as a list of strings, but the built-in estimators all need numeric input.  The standard way to deal with categorical features is **one-hot encoding**, also known as dummy variables.  In this approach, each category gets its own column in the feature matrix.  If the row has a given category, that column gets filled with a 1.  Otherwise, it is 0.\n",
    "\n",
    "The `ColumnSelectTransformer` from the previous question can be used to extract the categories column as a list of strings.  Scikit Learn provides [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which takes in a list of dictionaries.  It creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it.  Missing keys are filled with zeros.  Therefore, we need only build a transformer that takes a list strings to a dictionary with keys given by those strings and values one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will come in as a list of lists of lists.  Return a list of\n",
    "        # dictionaries corresponding to those inner lists.\n",
    "        ls = []\n",
    "        for li in X:\n",
    "            ls.append({i:1 for i in li[0]})\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should allow this to pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (DictEncoder().fit_transform([[['a']], [['b', 'c']]])\n",
    "        == [{'a': 1}, {'b': 1, 'c': 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a pipeline with your `ColumnSelectTransformer`, your `DictEncoder`, the `DictVectorizer`, and a regularized linear model, like `Ridge`, as the estimator.  This model will have a large number of features, one for each category, so there is a significant danger of overfitting.  Use cross validation to choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model, preprocessing, model_selection\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cst = ColumnSelectTransformer(['categories'])\n",
    "vect = DictVectorizer(sparse=False)\n",
    "v = vect.fit_transform(DictEncoder().fit_transform(cst.fit_transform(data)))\n",
    "av = vect.inverse_transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vect.vocabulary_)\n",
    "{k: vect.vocabulary_[k] for k in vect.vocabulary_.keys()[:20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, star_ratings, test_size=0.3)\n",
    "\n",
    "cols = ['categories']\n",
    "pipe = Pipeline([\n",
    "   ('cst', ColumnSelectTransformer(cols)) , # ColumnSelectTransformer\n",
    "    ('denc', DictEncoder()),\n",
    "    ('vect', DictVectorizer()),\n",
    "    ('tf_idf', TfidfTransformer() ),\n",
    "   ('ridge', linear_model.Ridge()) # Ridge Regression\n",
    "    ])\n",
    "\n",
    "cv = model_selection.ShuffleSplit(n_splits=20, test_size=0.2, random_state=42)\n",
    "param_grid = { \"ridge__alpha\": range(2, 8, 2) }\n",
    "category_est_cv = model_selection.GridSearchCV(pipe, param_grid=param_grid, cv=cv, \n",
    "                                                       scoring='r2')\n",
    "print dt.datetime.now()\n",
    "category_est_cv.fit(X_train, y_train)\n",
    "#category_est_cv.fit(data, star_ratings)\n",
    "category_est_cv_results = category_est_cv.cv_results_\n",
    "category_est_cv_score = pd.DataFrame.from_items(\n",
    "    [('alpha',[rec['ridge__alpha'] for rec in category_est_cv_results['params']]),\n",
    "     ('score',category_est_cv_results['mean_test_score'])])\n",
    "\n",
    "plt.plot(category_est_cv_score.alpha, category_est_cv_score.score, label=\"category_est_cv_score\")\n",
    "plt.xlabel('ridge__alpha')\n",
    "plt.ylabel('score')\n",
    "plt.legend(loc='lower center')\n",
    "plt.show()\n",
    "\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_est_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_est = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(cols)) , # ColumnSelectTransformer\n",
    "    ('denc', DictEncoder()),\n",
    "    ('vect', DictVectorizer(sparse=False)),\n",
    "    ('tf_idf', TfidfTransformer() ),\n",
    "   ('ridge', linear_model.Ridge(alpha=4)) # Ridge Regression\n",
    "    ])\n",
    "#category_est.fit(data,star_ratings)\n",
    "category_est.fit(X_train,y_train)\n",
    "category_est.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__category_model', category_est.predict)  # Edit to appropriate name\n",
    "#grader.score('ml__category_model', category_est_cv.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extension:* Some categories (e.g. Restaurants) are not very specific.  Others (Japanese sushi) are much more so.  One way to deal with this is with an measure call term-frequency-inverse-document-frequency (TF-IDF).  Add in a `sklearn.feature_extraction.text.TfidfTransformer` between the `DictVectorizer` and the linear model, and see if that improves performance.\n",
    "\n",
    "*Extension:* Can you beat the performance of the linear estimator with a\n",
    "non-linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attribute_model\n",
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them with one-hot encoding.  The `DictVectorizer` can do this, but only once we've flattened the dictionary to a single level, like so:\n",
    "```\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "\n",
    "Build a custom transformer that flattens the attributes dictionary.  Place this in a pipeline with a `DictVectorizer` and a regressor.\n",
    "\n",
    "You may find it difficult to find a single regressor that does well enough.  A common solution is to use a linear model to fit the linear part of some data, and use a non-linear model to fit the residual that the linear model can't fit.  Build a residual estimator that takes as an argument two other estimators.  It should use the first to fit the raw data and the second to fit the residuals of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import preprocessing, model_selection, linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn import ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data\n",
    "y = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttributeSelectTfmRecursive(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names='attributes'):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "        self.pr_dict = {1:'one', 2:'two', 3:'three', 4:'four'} # a dict for price range lookups\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def parse_dict(self, key, val):\n",
    "        l = []\n",
    "        if isinstance(val,dict):\n",
    "            for k, v in val.items():\n",
    "                if not isinstance(v,dict):\n",
    "                    l.append((key + '_'+ k,v))\n",
    "                else:\n",
    "                    self.parse_dict(key + '_'+ k, v)\n",
    "        else:\n",
    "            l.append((key,self.pr_dict[val]) if key == 'Price Range' else (key.title(),val))         \n",
    "        return l\n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = []\n",
    "        for attr in [dx.get(self.col_names) for dx in X]:\n",
    "            ls = []\n",
    "            for ke,va in attr.items():\n",
    "                ls += self.parse_dict(ke,va)\n",
    "            result.append({x[0]:x[1] for x in ls })\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator, residual_estimators):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.residual_estimators = residual_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.base_estimator.fit(X, y)\n",
    "        y_err = y - self.base_estimator.predict(X)\n",
    "        for est in self.residual_estimators:\n",
    "            est.fit(X, y_err)                     # fitting the error (y - y_pred) into KNNreg and then RandForestReg\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        all_ests = [self.base_estimator] + list(self.residual_estimators)\n",
    "        return np.array([est.predict(X) for est in all_ests]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, star_ratings, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt.datetime.now()\n",
    "r2_list = []\n",
    "for xnum in range(15,40,5):\n",
    "    for num in range(3,10,2):\n",
    "        ensemble_pipe = Pipeline([\n",
    "               ('ast', AttributeSelectTfmRecursive()),\n",
    "               ('vect', DictVectorizer(sparse=False)), \n",
    "               ('ensemble', EnsembleTransformer(\n",
    "                        linear_model.LinearRegression(),\n",
    "                        (KNeighborsRegressor(n_neighbors=num, algorithm='kd_tree'),           \n",
    "                         ensemble.RandomForestRegressor(min_samples_leaf=xnum)))), \n",
    "                ('blend', linear_model.LinearRegression())                       \n",
    "            ])\n",
    "        #ensemble_pipe.fit(X_train[:n], y_train[:n]) \n",
    "        ensemble_pipe.fit(X_train, y_train) \n",
    "        r2_list.append((ensemble_pipe.score(X_test,y_test),( num, xnum)))\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dt.datetime.now()\n",
    "attribute_est = Pipeline([\n",
    "   ('ast', AttributeSelectTfmRecursive()),\n",
    "   ('vect', DictVectorizer(sparse=False)), \n",
    "   ('ensemble', EnsembleTransformer(\n",
    "            linear_model.LinearRegression(),\n",
    "            (KNeighborsRegressor(n_neighbors=5, algorithm='kd_tree'),           \n",
    "             ensemble.RandomForestRegressor(min_samples_leaf=20)))), \n",
    "    ('blend', linear_model.LinearRegression())                       \n",
    "])\n",
    "#ensemble_pipe.fit(X_train[:n], y_train[:n]) \n",
    "attribute_est.fit(X_train, y_train) \n",
    "print '{},{} ==> {}'.format(5,20,attribute_est.score(X_test,y_test))\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (10,40) ==> 0.0876832516126\n",
    "print ensemble_pipe.score(X_test,y_test)\n",
    "sorted(r2_list, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__attribute_model', attribute_est.predict)  # Edit to appropriate name\n",
    "#grader.score('ml__attribute_model', ensemble_pipe.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('city_est.dill', 'w') as est:\n",
    "    dill.dump(city_est, est)\n",
    "    \n",
    "with open('lat_long_est.dill', 'w') as est:\n",
    "    dill.dump(lat_long_est, est)\n",
    "    \n",
    "with open('category_est.dill', 'w') as est:\n",
    "    dill.dump(category_est, est)\n",
    "\n",
    "with open('attribute_est.dill', 'w') as est:\n",
    "    dill.dump(attribute_est, est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('city_est.dill', 'r') as est:\n",
    "    city_est = dill.load(est)\n",
    "    \n",
    "with open('lat_long_est.dill', 'r') as est:\n",
    "    lat_long_est = dill.load(est)\n",
    "    \n",
    "with open('category_est.dill', 'r') as est:\n",
    "    category_est = dill.load(est)\n",
    "    \n",
    "with open('attribute_est.dill', 'r') as est:\n",
    "    attribute_est = dill.load(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full_model\n",
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to an estimator, we will have to turn them into transformers.  (A pipeline can contain at most a single estimator.)  Build a custom `ModelTransformer` class that takes an estimator as an argument.  When `fit()` is called, the estimator should be fit.  When `transform()` is called, the estimator's `predict()` method should be called, and its results returned.\n",
    "\n",
    "Note that the output of the `transform()` method should be a 2-D array with a single column, in order for it to work well with the Scikit Learn pipeline.  If you're using Numpy arrays, you can use `.reshape(-1, 1)` to create a column vector.  If you are just using Python lists, you will want a list of lists of single elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EstimatorTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        # What needs to be done here?\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the stored estimator.\n",
    "        # Question: what should be returned?\n",
    "        # None will be returned without using the 'return' keyword\n",
    "        self.estimator.fit(X,y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Use predict on the stored estimator as a \"transformation\".\n",
    "        # Be sure to return a 2-D array.\n",
    "        #print type(X)\n",
    "        return [[x] for x in self.estimator.predict(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.670054772869717],\n",
       " [3.7447916666666665],\n",
       " [3.7447916666666665],\n",
       " [3.7447916666666665],\n",
       " [3.7447916666666665]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_trans = EstimatorTransformer(city_est)\n",
    "city_trans.fit(data, star_ratings)\n",
    "assert ([r[0] for r in city_trans.transform(data[:5])]\n",
    "        == city_est.predict(data[:5]))\n",
    "city_trans.transform(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-09 16:09:07.038317\n",
      "2018-02-09 16:12:10.011209\n"
     ]
    }
   ],
   "source": [
    "print dt.datetime.now()\n",
    "lat_long_trans = EstimatorTransformer(lat_long_est)\n",
    "lat_long_trans.fit(data, star_ratings)\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[[ 4.03676471]\n",
      " [ 3.58088235]\n",
      " [ 3.57352941]\n",
      " [ 3.58823529]\n",
      " [ 3.57352941]\n",
      " [ 3.56617647]\n",
      " [ 3.32352941]\n",
      " [ 3.33088235]\n",
      " [ 3.39705882]\n",
      " [ 3.38235294]]\n",
      "2018-02-09 16:12:57.902881\n"
     ]
    }
   ],
   "source": [
    "print lat_long_trans.transform(data[:10])\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-09 15:59:21.579211\n",
      "2018-02-09 15:59:22.551744\n"
     ]
    }
   ],
   "source": [
    "print dt.datetime.now()\n",
    "category_trans = EstimatorTransformer(category_est)\n",
    "category_trans.fit(data, star_ratings)\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[[3.2671231067549185], [3.0575147644688747], [3.2245305166167757], [3.2856027840542259], [3.3059986595917499], [3.5152689546274112], [4.0953892314246305], [3.9409850379812381], [4.1046161392687246], [3.3867762665891852]]\n"
     ]
    }
   ],
   "source": [
    "print category_trans.transform(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-09 16:00:22.220097\n",
      "2018-02-09 16:04:04.649432\n"
     ]
    }
   ],
   "source": [
    "print dt.datetime.now()\n",
    "attribute_trans = EstimatorTransformer(attribute_est)\n",
    "attribute_trans.fit(data, star_ratings)\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = attribute_trans.transform(data[:10])\n",
    "np.array(t).shape\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `ModelTransformer` for each of the previous four models. Combine these together in a single feature matrix with a\n",
    "[FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print lat_long_est.get_params\n",
    "print city_est.get_params\n",
    "print category_est.get_params\n",
    "print attribute_est.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-09 16:35:57.144914\n",
      "2018-02-09 16:42:29.604815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "import datetime as dt\n",
    "print dt.datetime.now()\n",
    "union = FeatureUnion([\n",
    "    # FeatureUnions use the same syntax as Pipelines\n",
    "    ('city_trans', EstimatorTransformer(city_est)),\n",
    "    ('lat_long_trans', EstimatorTransformer(lat_long_est)),\n",
    "    ('category_trans', EstimatorTransformer(category_est)),\n",
    "    ('attribute_trans', EstimatorTransformer(attribute_est))\n",
    "    ])\n",
    "union.fit(data, star_ratings)\n",
    "print dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cst = ColumnSelectTransformer(['city','attributes','categories','longitude','latitude'])\n",
    "#cst.fit_transform(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-110ca1680184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print type(arr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#union.transform(arr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0munion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print data[:1][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#[r[0] for r in union.transform(data[:5])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#arr = np.array(cst.fit_transform(data[:10]))\n",
    "#print arr.shape\n",
    "#print type(arr)\n",
    "#union.transform(arr)\n",
    "union.transform(data[:3])\n",
    "#print data[:1][0]\n",
    "#[r[0] for r in union.transform(data[:5])]\n",
    "#union.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a feature matrix with four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "[[u'Phoenix' {u'By Appointment Only': True}\n",
      "  [u'Doctors', u'Health & Medical'] -111.983758 33.499313]]\n"
     ]
    }
   ],
   "source": [
    "#union.fit(data, star_ratings)\n",
    "#trans_data = union.transform(cst.fit_transform(data[:10]))\n",
    "test_data = np.array(cst.fit_transform(data[:1]))\n",
    "print test_data.shape\n",
    "print test_data\n",
    "#print type(trans_data)\n",
    "#print trans_data.shape\n",
    "#assert trans_data.shape == (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data[:1] # u'Phoenix De Forest\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use a pipeline to combine the feature union with a linear regression (or another model) to weight the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-8e0a08bc76ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"linreg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     ])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfull_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_transformer_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "full_est = Pipeline([\n",
    "    (\"union\", union),\n",
    "    (\"linreg\", linear_model.LinearRegression(fit_intercept=True))\n",
    "    ])\n",
    "full_est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.score('ml__full_model', full_est.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extension:* By combining our models with a linear model, we will be unable to notice any correlation between features.  We don't expect all attributes to have the same effect on all venues.  For example, \"Ambiance: divey\" might be a bad indicator for a restaurant but a good one for a bar.  Nonlinear models can pick up on this interaction.  Replace the linear model combining the predictions with a nonlinear one like [RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor).  Better yet, use the nonlinear model to fit the residuals of the linear model.\n",
    "\n",
    "The score for this question is just a ratio of the score of your model to the score of a reference solution.  Can you beat the reference solution and get a score greater than 1.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
